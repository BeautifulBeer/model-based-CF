{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "from numba import njit, prange, types\n",
    "from numba.typed import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import preprocessing\n",
    "import training\n",
    "import test\n",
    "import tools\n",
    "import importlib\n",
    "\n",
    "# reload moudules\n",
    "importlib.reload(tools)\n",
    "importlib.reload(test)\n",
    "importlib.reload(training)\n",
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing GroupLens Dataset\n",
    "For testing a model, divide dataset into two parts, training and test. The original dataset is sequential, hence we should shuffle the dataset randomly. We will build an adjacency matrix for the rating matrix because of the memory limits. The whole rating matrix, about **`280000 X 58000`**, cannot be loaded on memory practically. \"userMapper\" and \"movieMapper\" are a hashmap to convert an user and a movie ids into continuous natural number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training - test ratio\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "# for testing, shuffle dataset\n",
    "if not os.path.isfile('shuffled_ratings.csv'):\n",
    "    shuffle_data()\n",
    "    \n",
    "ratings = pd.read_csv('shuffled_ratings.csv')\n",
    "split_bound = int(ratings.shape[0] * train_ratio)\n",
    "\n",
    "print(f\"Load shuffled data : {ratings.shape}\")\n",
    "print(f\"Train / Test ratio : {train_ratio} / {test_ratio}\")\n",
    "print(f\"Split data : [0:{split_bound}] , [{split_bound}:{ratings.shape[0]}]\")\n",
    "\n",
    "train_data = ratings[:split_bound]\n",
    "test_data = ratings[split_bound:]\n",
    "train_data.reset_index(inplace=True, drop=True)\n",
    "test_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# training data\n",
    "print(\"##### Get mapper and adjacency matrix\")\n",
    "start_time = time.time()\n",
    "userMapper = userIdMapper(train_data)\n",
    "print(f\"userMapper processing Time : {time.time() - start_time}\")\n",
    "start_time = time.time()\n",
    "movieMapper = movieIdMapper(train_data)\n",
    "print(f\"movieMapper processing Time : {time.time() - start_time}\")\n",
    "start_time = time.time()\n",
    "user_item = build_adjacency_matrix(train_data, userMapper, movieMapper)\n",
    "print(f\"adjacency matrix processing Time : {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model\n",
    "\n",
    "LFM(Latent Factor Model) is a modeling assuming that there are latent factors to determine an entity. We set 20 dimension features to represent characteristics for each user and movie. These features are the unknown characteristics to calculate a movie rating of a user. This is driven by the assumption that \"Some factors absolutely affect to determine rating but, I do not know what the factors are\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative Filtering (Model-based), Trainig\n",
    "epoch = 1\n",
    "learning_rate = 0.005\n",
    "regular_term = 0.1\n",
    "latent_dim = 20\n",
    "\n",
    "user_count = len(userMapper.keys())\n",
    "movie_count = len(movieMapper.keys())\n",
    "\n",
    "[fm_users, fm_movies, train_error] = tools.load_files(user_count, movie_count, latent_dim)\n",
    "\n",
    "for i in range(epoch):\n",
    "    start_time = time.time()\n",
    "    lossValue = training.training(user_item, fm_users, user_count, fm_movies, learning_rate, regular_term)\n",
    "    print(f\"Time : {time.time() - start_time}\")\n",
    "    train_error.append(lossValue)\n",
    "    print(f\"###### epoch {i} {lossValue}\")\n",
    "    \n",
    "plt.plot(train_error)\n",
    "plt.ylabel('train errors per epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "After training model, we should check whether model is trained well. We already splited the dataset to test the model, therefore we just use it! We measure performance of the model using RMSE(Root Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "test_err = test_error(test_data)\n",
    "print(f\"Time : {time.time() - startTime}\")\n",
    "print(f\"Test dataset error : {test_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files(fm_users, fm_movies, train_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(50, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
